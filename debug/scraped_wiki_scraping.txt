================================================================================
SCRAPED CONTENT FROM: https://en.wikipedia.org/wiki/Web_scraping
SCRAPED ON: 2025-10-17 11:18:29
================================================================================

TITLE:
Web scraping - Wikipedia

HEADINGS:
  - Contents
  - History
  - Techniques
  - Human copy-and-paste
  - Text pattern matching
  - HTTP programming
  - HTML parsing
  - DOM parsing
  - Vertical aggregation
  - Semantic annotation recognizing
  - Computer vision web-page analysis
  - AI-powered document understanding
  - Legal issues
  - United States
  - European Union
  - Australia
  - India
  - Methods to prevent web scraping
  - See also
  - References

MAIN CONTENT:
Scraping a web page involves fetching it and then extracting data from it. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Having fetched, extraction can take place. The content of a page may be parsed , searched and reformatted, and its data copied into a spreadsheet or loaded into a database. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be finding and copying names and telephone numbers, companies and their URLs, or e-mail addresses to a list (contact scraping).

As well as contact scraping , web scraping is used as a component of applications used for web indexing , web mining and data mining , online price change monitoring and price comparison , product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection , research, tracking online presence and reputation, web mashup , and web data integration .

Newer forms of web scraping involve monitoring data feeds from web servers. For example, JSON is commonly used as a transport mechanism between the client and the web server.

There are methods that some websites use to prevent web scraping, such as detecting and disallowing bots from crawling (viewing) their pages. In response, web scraping systems use techniques involving DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing.

In December 1993, the first crawler-based web search engine, JumpStation , was launched. As there were fewer websites available on the web, search engines at that time used to rely on human administrators to collect and format links. In comparison, Jump Station was the first WWW search engine to rely on a web robot.

Web scraping is the process of automatically mining data or collecting information from the World Wide Web. It is a field with active developments sharing a common goal with the semantic web vision, an ambitious initiative that still requires breakthroughs in text processing, semantic understanding, artificial intelligence and human-computer interactions .

The simplest form of web scraping is manually copying and pasting data from a web page into a text file or spreadsheet. Sometimes even the best web-scraping technology cannot replace a human's manual examination and copy-and-paste, and sometimes this may be the only workable solution when the websites for scraping explicitly set up barriers to prevent machine automation.

A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression -matching facilities of programming languages (for instance Perl or Python ).

By using a program such as Selenium or Playwright , developers can control a web browser such as Chrome or Firefox wherein they can load, navigate, and retrieve data from websites. This method can be especially useful for scraping data from dynamic sites since a web browser will fully load each page. Once an entire page is loaded, you can access and parse the DOM using an expression language such as XPath .

There are several companies that have developed vertical specific harvesting platforms. These platforms create and monitor a multitude of "bots" for specific verticals with no "man in the loop" (no direct human involvement), and no work related to a specific target site. The preparation involves establishing the knowledge base for the entire vertical and then the platform creates the bots automatically. The platform's robustness is measured by the quality of the information it retrieves (usually number of fields) and its scalability (how quick it can scale up to hundreds or thousands of sites). This scalability is mostly used to target the Long Tail of sites that common aggregators find complicated or too labor-intensive to harvest content from.

Uses advanced AI to interpret and process web page content contextually, extracting relevant information, transforming data, and customizing outputs based on the content's structure and meaning. This method enables more intelligent and flexible data extraction, accommodating complex and dynamic web content.

Leaving a few cases dealing with IPR infringement, Indian courts have not expressly ruled on the legality of web scraping. However, since all common forms of electronic contracts are enforceable in India, violating the terms of use prohibiting data scraping will be a violation of the contract law. It will also violate the Information Technology Act, 2000 , which penalizes unauthorized access to a computer resource or extracting data from a computer resource.

The administrator of a website can use various measures to stop or slow a bot. Some techniques include:

LIST ITEMS:
  - (Top) 1 History
  - 2.1 Human copy-and-paste 2.2 Text pattern matching 2.3 HTTP programming 2.4 HTML parsing 2.5 DOM parsing 2.6 Vertical aggregation 2.7 Semantic annotation recognizing 2.8 Computer vision web-page analysis 2.9 AI-powered document understanding
  - 3.1 United States 3.2 European Union 3.3 Australia 3.4 India 4 Methods to prevent web scraping 5 See also 6 References
  - Blocking an IP address either manually or based on criteria such as geolocation and DNSRBL . This will also block all browsing from that address. Disabling any web service API that the website's system might expose. Bots sometimes declare who they are (using user agent strings ) and can be blocked on that basis using robots.txt ; ' googlebot ' is an example. Other bots make no distinction between themselves and a human using a browser. Bots can be blocked by monitoring excess traffic.
  - Locating bots with a honeypot or other method to identify the IP addresses of automated crawlers.
  - Because bots rely on consistency in the front-end code of a target website, adding small variations to the HTML/CSS surrounding important data and navigation elements would require more human involvement in the initial set up of a bot and if done effectively may render the target website too difficult to scrape due to the diminished ability to automate the scraping process. Websites can declare if crawling is allowed or not in the robots.txt file and allow partial access, limit the crawl rate, specify the optimal time to crawl and more.
  - This page was last edited on 2 October 2025, at 23:54 &#160;(UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.

IMAGE ALT TEXTS:
  - Wikipedia
  - The Free Encyclopedia
  - Globe icon.
  - icon
  - Wikimedia Foundation
  - Powered by MediaWiki

LINKS (unique, max 50):
  - Jump to content
  - Edit links
  - Data scraping
  - Scraper site
  - verification
  - improve this article
  - adding citations to reliable sources
  - "Web scraping"
  - news
  - newspapers
  - books
  - scholar
  - JSTOR
  - Learn how and when to remove this message
  - extracting data
  - websites
  - World Wide Web
  - Hypertext Transfer Protocol
  - web crawler
  - database
  - spreadsheet
  - retrieval
  - analysis
  - parsed
  - contact scraping
  - web indexing
  - web mining
  - data mining
  - price comparison
  - website change detection
  - web mashup
  - web data integration
  - Web pages
  - HTML
  - XHTML
  - end-users
  - market research
  - data feeds
  - JSON
  - computer vision
  - natural language processing
  - birth of the World Wide Web
  - World Wide Web Wanderer
  - JumpStation
  - Salesforce
  - eBay
  - instructions or advice
  - rewrite the content
  - move
  - Wikiversity

================================================================================
END OF SCRAPED CONTENT
================================================================================
